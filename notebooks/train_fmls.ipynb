{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a recurrent neural net on FML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Today, while guitaring my morning advice with my time of walking girl. I lost bored, oddly buriover at a 2%, my boyfriend dumped me next to me. FML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### -- FMLBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bots have tough lifes too, so we should listen to their sorrows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of contents:\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Imports](#Imports)\n",
    "3. [Load FML data](#Load-FML-data)\n",
    "4. [Parameter settings](#Parameters)\n",
    "5. [Training the model](#Training-the-model)\n",
    "6. [Sampling from the model](#Sampling-from-the-model)\n",
    "7. [Saving the model](#Saving-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Spaghetto</b> provides tools for training recurrent neural networks (RNNs) on sequences and to generate sequences. This fits especially well to text. \n",
    "\n",
    "The idea for Spaghetto was sparked by Andrej Karpathy's [blog post](karpathy.github.io/2015/05/21/rnn-effectiveness). While he provided working code, it was written in Lua, which fewer people are familiar with than Python. Spaghetto's interface closely resembles the excellent [nolearn](https://github.com/dnouri/nolearn) library's interface, which in turn is mostly the same as [scikit learn](scikit-learn.org)'s. The layers are taken from the [Lasagne](https://github.com/Lasagne/Lasagne) library. Further inspiration was drawn from [Shawn Tan](https://github.com/shawntan/theano-nlp)'s [Theano](deeplearning.net/software/theano)-based implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from lasagne.layers import Layer, InputLayer, EmbeddingLayer\n",
    "from lasagne import nonlinearities\n",
    "from lasagne.updates import rmsprop\n",
    "from lasagne.layers import GRULayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spaghetto.model import RNN\n",
    "from spaghetto.utils import TokenEncoder\n",
    "from spaghetto.recurrent import RNNDenseLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FML data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data from FML. Unfortunately, I don't think I can provide this data to you. \n",
    "\n",
    "Original data source: [FML](www.fmylife.com)\n",
    "\n",
    "Each FML is stored in a row in the text file. Therefore, _X_ will be a list of strings, which each element corresponding to an FML. This is a good representation if the lines are independent of each other. However, there are cases when this is not true, for example when we want the RNN to train on a book. In that case, our _X_ should just be one long string. Spaghetto will then automatically break this string into lines for us.\n",
    "\n",
    "To recapitulate:\n",
    "* Use one long string as input if the content of the data is related.\n",
    "* Use a list of strings if the elements are independent of one another, as is the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = open('/home/bbossan_dev/Downloads/fmls.txt').readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is encoded in latin-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = [x.decode('latin-1') for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what a sample FML looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today, my brother told me to, \"Stop bitching and get over it\" after I complained of pain from my stomach after invasive surgery. This from the guy who spends multiple hours a day playing Halo and whining about the stupid ways he got killed. FML \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN will need to know how to encode the text. For that, we use the spaghetto TokenEncoder. Internally, this converts the characters into integers that are used to look up the character's embedding from the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = TokenEncoder().fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also tell the encoder not to split on a character-level but on a word level by specifying the _separator_ argument to be space, i.e. _encoder = TokenEncoder(separator=' ')_. You will generally get better results from a character-level encoding, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify the parameters for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "num_units = 200\n",
    "max_len_line = 300\n",
    "update = rmsprop\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meanings are:\n",
    "* _embedding_size_: The size of character embeddings.\n",
    "* _num_units_: Number of units used for the hidden state of the network.\n",
    "* _max_len_line_: Cut a line if it contains more than this many tokens. This is mainly there because there might be some outliers with very high length, which would lead to all other samples in the batch being padded to that lenght, which in turn lowers performance.\n",
    "* _update_: The updating rule for the parameters; for RNNs, RMSprop seems be the default.\n",
    "* _learning_rate_: The learning rate of the updater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify the architecture of the net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [\n",
    "    (InputLayer, {}),\n",
    "    (EmbeddingLayer, {'output_size': embedding_size}),\n",
    "    (GRULayer, {'num_units': num_units}),\n",
    "    (GRULayer, {'num_units': num_units}),\n",
    "    (RNNDenseLayer, {'nonlinearity': nonlinearities.identity}),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers are:\n",
    "* _InputLayer_: Always begin with an input layer. Spaghetto takes care of setting all required parameters of the InputLayer automatically for you.\n",
    "* _EmbeddingLayer_: Each character/token has its own embedding, which is stored in the embedding layer. Spaghetto automatically sets the input size for us. This layer could be used to find token similarities, similar to [word2vec](https://code.google.com/p/word2vec) word embeddings. This of course makes more sense when you tokenize on words and not on characters.\n",
    "* 2 _GRULayers_: This is our recurrent layer of choice here. We could also use Lasagne's LSTMLayer, which might improve the outcome at the cost of slower training time. You can also try to use more recurrent layers, or different numbers of units per recurrent layer.\n",
    "* _RNNDenseLayer_: This is a convenient DenseLayer provided by Spaghetto that preserves the 3-dimensional shape of recurrent layers (batch size x time x number of units) instead of flattening the output to 2 dimensions, as Lasagne's DenseLayer would. We should set the output nonlineariy to _identity_ because Spaghetto automatically applies the correct nonlinearity afterwards. Spaghetto also takes care to automatically set the number of output units for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we specify the recurrent neural network itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = RNN(\n",
    "    layers,\n",
    "    encoder=encoder,\n",
    "    verbose=1,\n",
    "    updater=partial(update, learning_rate=learning_rate),\n",
    "    max_epochs=15,\n",
    "    max_len_line=max_len_line,\n",
    "    eval_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are:\n",
    "* _layers_: First and most importantly, we pass the layers we just defined.\n",
    "* _encoder_: The net needs to know how to encode the data, so we pass the encoder that we fitted above.\n",
    "* _verbose_: Set this value to greater than 0 to receive some useful information about the net during training.\n",
    "* _updater_: The update rule.\n",
    "* _max_epochs_: Maximum number of epochs to train.\n",
    "* _max_len_line_: The maximum number of characters per line.\n",
    "* _eval_size_: Proportion of data held back for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One special parameter for training that does not exist in sklearn or nolearn is _on_nth_batch_. The Spaghetto RNN has a call back after each batch, which allows you to get more frequent feedback from the model. Waiting for a whole epoch to finish can be tedious, especially if you train on a lot of data. On the other hand, getting feedback after each batch is too frequent for most cases. Therefore, the _on_nth_batch_ parameter allows you to regulate how often you want to get feedback. With _on_nth_batch=250_, that means that you get feedback each 250th batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you will receive some information from the model, as you would when using nolearn. Here the information is:\n",
    "* _epoch_: How many epochs have passed (i.e. full passes through the training data).\n",
    "* _train perpl._: The perplexity on the training set. Lower is better, with 1 being the minimum. If a new best value is achieved, it is colored <font color=\"cyan\">cyan</font>\n",
    "* _valid perpl._: The perplexity on the hold of validation set. If a new best value is achieved, it is colored <font color=\"green\">green</font>.\n",
    "* _train/valid_: Proportion of train to validation perplexity. If this becomes too low, it means that your model overfits. How about adding a dropout layer?\n",
    "* _duration_: Time it took to train the last batches.\n",
    "* _sample_: Especially with RNNs that can take quite some time to train, it is important to monitor the progress during training. In addition to the perplexity, a more *human-friendly* way to achieve this is to look at samples generated by the RNN. Are they gibberish or do they look more and more reasonable as training progresses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bbossan_dev/anaconda3/envs/spaghetto/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/scan_module/scan.py:1019: Warning: In the strict mode, all neccessary shared variables must be passed as a part of non_sequences\n",
      "  'must be passed as a part of non_sequences', Warning)\n",
      "/home/bbossan_dev/anaconda3/envs/spaghetto/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/scan_module/scan_perform_ext.py:135: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  from scan_perform.scan_perform import *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Compiling functions ...\n",
      "   ... finished compilation.\n",
      "\n",
      "  epoch    train perpl.    valid perpl.    train/valid  duration    sample\n",
      "-------  --------------  --------------  -------------  ----------  ------------------------------------------------------------------------------------------------------\n",
      "      1        \u001b[36m14.05023\u001b[0m        \u001b[32m13.94870\u001b[0m        1.00728  108.52s     Today, I I whemos haok oth h\" I wathrey boeithI honl harkg I vhe theol ont an dtor wuttar7 and, FML\n",
      "      1         \u001b[36m3.90302\u001b[0m         \u001b[32m4.03950\u001b[0m        0.96621  114.22s     Today, my butel, I girled mirur gomelliendyi't gertsery, witt gloset'ss. Oil_ater, writeth, so deower, octior onreat in al, it lookked a liglried, and, intiouttyrarmiends, oneler, sone bablitoldor mirster, ous,b'ssserp, arriend, antalle woilled lastsicarpedsse of my seecid it really pelletuous.\" So\n",
      "      1         \u001b[36m3.78602\u001b[0m         \u001b[32m3.74330\u001b[0m        1.01140  108.59s     Today, I had buy. He walked my date canalbicep)ed looked and gatting. FML\n",
      "      2         \u001b[36m2.93258\u001b[0m         \u001b[32m3.01190\u001b[0m        0.97366  103.07s     Today, and got pationder, I for cregented into negteching ad into analle, my get. She cocchending into me and feeted for \"coeter\"itedly siffwen. \"U picmpled going me.\" Me her what reached up. FML\n",
      "      2         \u001b[36m2.61560\u001b[0m         \u001b[32m2.66600\u001b[0m        0.98108  111.58s     Today, , was wasted to pooplems uf of going was a keckressort. My hort fordoe to compleral lifzor rivized tomant, so, so had to the sast was pot-to hard down, while was werested to last under to free that getfinwer receptly younger. FML\n",
      "      2         2.95060         2.94410        1.00221  115.01s     Today, I grabbing walking on with my boy peariby really loying his walleg, I was wacked in call and is nazing him because I am about a prews bittraed, down at a gamplbs and I was expling vising my licated and dumped my baned said, \"furable on by 5 year tyas numberave in said, \"Nh, opened the Galkin\n",
      "      3         \u001b[36m2.53931\u001b[0m         \u001b[32m2.60160\u001b[0m        0.97605  95.53s      Today, it would realive for meetint scustervious face from still that I feelice Jeached up big into an agnes home. As is 95 better to a. FML\n",
      "      3         \u001b[36m2.37677\u001b[0m         \u001b[32m2.42020\u001b[0m        0.98205  114.36s     Today, after turned to telly sounssionly nose, hoby to a sign sluNs and time to this, old with itgotter. Not orgained, and I took at a boss and now \"Yy: by Ciny!\" Hour are because I snuube blown for us. To couchtict, I passenced, \"I'd yourted a boyfriend, \"Wo your really coffee boyfriend\" all boyfr\n",
      "      3         2.67230         2.67250        0.99991  112.93s     Today, I wiuked my own I thought you such his she showing a pictures ses over that paying to spend me. I the bab having sway \"at?L Just school. She was having Holled me. Gather asked \"But how he realioping \"Oan having a legresset mom. \" FML\n",
      "      4         \u001b[36m2.37625\u001b[0m         2.43080        0.97755  94.01s      Today, feet pain, I jacked \"I\"OPDTD, \"The wife so I did \"sex OI005 doing, \"Awe she what the stree walked and I was walk the olds and her hold wors lipe, his doing the couses when they didn't even me. She then iPnoposed scree, she's love in the servity to \"sue sterical way that \"Iont.  for the hear\n",
      "      4         \u001b[36m2.28320\u001b[0m         \u001b[32m2.34180\u001b[0m        0.97499  112.33s     Today, I was at my dog deor clothes people from my back three me. Low tA watch so choaching bought really accurized To anote) through the status rubber what would sex with the t*, mave been money he has my mother-by invited on my chick me says. \" FML\n",
      "      4         2.50760         2.51720        0.99621  114.95s     Today, I tried to screen my husband has passive tampon on there and every dils by my noob. And but hit I was on the facall, shot into the shoes and was how I am not. I rust hat there walk out with by my brandwhing to hour of tranm of my boyfriend to stop still to wait bus months. After suit taking\n",
      "      5         2.28570         2.34330        0.97544  86.98s      Today, my daughter came home 1 after the very sen to sappes wife's weekere and my husband and my new year-old stand and he wanted a pare when we need and had penis and get a back because my husband and face\". I and he wanted to confused screan I've exple epensed the trap where she mept bigger sex o\n",
      "      5         \u001b[36m2.23278\u001b[0m         \u001b[32m2.29370\u001b[0m        0.97345  114.36s     Today,  it was falling to later typendeath after the ago of tonigating up to make me for a girl relationshacious and tell the call I hour chairst my back with the ih on hour right to roll a guy I can't that a case he clotled ofn off after myself up a came over to umped him to the upguan we in the k\n",
      "      5         2.40230         2.42660        0.99000  113.55s     Today, I was just dinner. Mot down, so exement that vertist, so does is. Lishing send to let over to say, Bost exeatedly chumpess, I three bloody couple of the temp of a job but took a pob of fire the onight. I've got down to his dates gotter uddative. FML\n",
      "      6         \u001b[36m2.22896\u001b[0m         2.29640        0.97062  77.85s      Today,, after me through the messI suirred from let her if enesed jiw diagnosed, \"She's 15 years, \"Yying from the intense, they'd been divor't wear have servey. \" he had left the shower?\" I just asthat it from a year. My dated sister's E\n",
      "      6         \u001b[36m2.19959\u001b[0m         \u001b[32m2.26050\u001b[0m        0.97305  114.62s     Today, my boyfriend convinced me of Things in my eye and roking on the Epe can. Meing an bitgest owessive by a glass city from the lift cheims of jations for the little suit 7 minutes to come to the month to insful month of me to am sorry to be a hoping milds in my homeje getting months planning ov\n",
      "      6         2.32040         2.35210        0.98651  115.10s     Today, my car was a vitten. I had to pump A bunch for the brom from love to retchen up and dreck out of the closest of drueknable, and I could go to a cute boobs to have a portrol around around advanetically come cruting, I find my place to a power had posted shouting her pictures some pumps. FML\n",
      "      7         \u001b[36m2.18369\u001b[0m         2.26100        0.96583  68.55s      Today, it's me-a hugher remember, very very mid-disneoss on and thinking, \"gurding the Show, I'm been charged, I said, \"Can I've regical drunk!\" FML\n",
      "      7         2.18420         \u001b[32m2.25390\u001b[0m        0.96911  111.01s     Today,  I discovered that while my mother's baw in the zoilb namer-serious for \"toye, IS 1Q, that surposs\" on her current size \"hilar chased me \"ushed in yard. \" I thought it would be under my friend's head. FML\n",
      "      7         2.25000         2.28530        0.98458  114.20s     Today, I had to the week when the girl I was helf company out of home, started problem. We left for my house. In this result, COW, \"Pet of something so eyem to have her new cover if I wants to make you. \" A presentely Boys left mother explating for. I couldn't mustoment wrong way. FML\n",
      "      8         \u001b[36m2.16019\u001b[0m         \u001b[32m2.24470\u001b[0m        0.96235  59.84s      Today, my family if my new orgoniner and empty all night is on him. FML\n",
      "      8         2.16470         \u001b[32m2.24180\u001b[0m        0.96561  114.43s     Today, the family begom the enough for then accused me her kneer anthiebs turned in tohe freaked to carry the mentiare expected me to be me a 26th station week. I cried because I'm serves in my seat great cameration. I have a new clean pregnancy related practice stay her leg by the back out horron\n",
      "      8         2.19650         \u001b[32m2.22940\u001b[0m        0.98523  114.88s     Today, I went to the drops and I got to a girl I near-long as a bald and an hour. I'd meant throw a nine jump I am I wasn't on the fire was a goddandake keys wear. I got a real peect from a night land and at the onight because I left me an antimating with about evening landuated by paning a dmer th\n",
      "      9         \u001b[36m2.14607\u001b[0m         2.23010        0.96232  59.86s      Today, after getting in the only really phone university are name to eyvergitle walking me for when they could be whenever I went on an unenaw with a gate, but thinks a hot of having to the cotay from. When I arrived,' do you who pointed out, then even have to break up to use the Pather to the moth\n",
      "      9         \u001b[36m2.14452\u001b[0m         2.23210        0.96078  114.51s     Today, my cockroat making up and said I could be aler&ies hand off, but had the movie with my breed hell, stim time home in a baby is still as she says a strip of work about the Domach with Lave me while after our makes me in my facally mear ristratement while that I missed his mother him if we don\n",
      "      9         2.15310         \u001b[32m2.18630\u001b[0m        0.98481  111.14s     Today, I realized to turn our foots in the coworman of there. She found out so proud drunk for found texts often tables. My friends had been 9\" dollars picture student's Cister, ok his bank was on fach to start spot. FML\n",
      "     10         \u001b[36m2.11938\u001b[0m         2.20200        0.96246  52.64s      Today, I used the man a jine having a having side little picked up house. Everyone is in local chemperity meal away that I was talking almost into the Cill . because icsad to talk to the same house, only too smell enough to steal, so I can't even middliate in the highway to tell my \"look rattle int\n",
      "     10         2.13580         2.23250        0.95667  113.07s     Today, it's my downstairs in my house until the possia. So taking to me in Facebook, I turn my favorite manlider so to do it all apologised. Afraying the riding of \"Fool claying horry wouldn't work oat? You yelling right in the kitchen to take its in the accident?\" FML\n",
      "     10         2.11980         \u001b[32m2.15560\u001b[0m        0.98336  114.25s     Today, I had a call and schedule of tease a customer repair at a okay stal stop larents of wall trying to find. Turns out my vagina said to sees you for my new little night next to me any with a history hat, and use this dance and was out that Justomer after he was the subwairs bage a taste apmos o\n",
      "     11         \u001b[36m2.09425\u001b[0m         2.16970        0.96523  40.57s      Today, my boyfriend was using a spider comfortable from a fat catorps of nowhere who how having Haraal. He was at a name waiting. FML\n",
      "     11         2.13470         2.23910        0.95338  110.12s     Today, I helped at Tenco when the due to collected my manager. As out why I was clubbing love flying when one of waiting recentle everything help me. I commented into her place. FML\n",
      "     11         \u001b[36m2.07929\u001b[0m         \u001b[32m2.12810\u001b[0m        0.97708  112.34s     Today, I had a $330 to meeting a pennis at about us. How closer to only Nrack of co-ends while I wore about 3 months, getting freezing unable to my girlfriend and her boss. Too bad I'm a toilet with aid freeze $50T. They'd said nothinet. FML\n",
      "     12         2.08180         2.16260        0.96261  38.78s      Today, I was turn to be computer baczeme curally that I was putstled about the but choar three motires old \"reap. \" I'm painting my friend shat pather the \"atte\" at Facebook-that I namem is that I cut. It was a town at the car, the only think that I donspite the guy that I'm choking because I after\n",
      "     12         2.13320         2.24490        0.95027  109.39s     Today, I waited for my Loweterly insomedity causing me. For so I assmassed my headphosposed the nest of the bathroom in what they dollard sweet underwear on. FML\n",
      "     12         \u001b[36m2.04742\u001b[0m         \u001b[32m2.09400\u001b[0m        0.97775  112.79s     Today, I had to drive dofe for the pinch take any name in happars, including it and forced to strand for a long day. Today, and I only green drive. It said : pretend and winked it without a night. My school far age if I can't. I had to taped to show a voicema. FML\n",
      "     13         2.07980         2.16510        0.96058  29.54s      Today, I got a pob that I decided to Christhat that I realized that only thought my dryer than I don't know the student mind I shoved apparently there for it. Apparently, broken, flueboked to have me with arrested from my fritos. FML\n",
      "     13         2.12780         2.24630        0.94727  108.54s     Today, I was brights. I suddenly stated to told her it and heard to enjoy him. It's going to visit. FML\n",
      "     13         \u001b[36m2.02442\u001b[0m         \u001b[32m2.07630\u001b[0m        0.97501  110.79s     Today, I received a very loud for a delivering store wall as out I've been in the playing food cake. Two turned out I'm not getting out of my co-surn. It's greeted all of them for me at supposed to leave from under. FML\n",
      "     14         2.06970         2.14230        0.96610  20.15s      Today, my family movies to humave it. I said, \"Bescen aing details about to, and swallow from a good proud my Mondore. \" FML\n",
      "     14         2.12140         2.23840        0.94774  112.36s     Today, while roommate, my wife so laughing you condollol, I swearned to sign someone a cereal hold month. How door still minutes, our kids, broke. FML\n",
      "     14         \u001b[36m2.00866\u001b[0m         2.07920        0.96608  120.41s     Today, my mother taxpsy to go to the bathroom. This is gives that fight-fell was fluebuon: whiquholy gave for stood off there. Suddenly are realizing, my boyfriend believed whatever happy believes while having me for my guitar. FML\n",
      "     15         2.05870         2.15730        0.95430  14.32s      Today, I finally hurt distance search for a piepny from the bathroom, when a distoreted attempt. I had while at a look dishes into a huge smell student for animalia from a spot at one. FML\n",
      "     15         2.11510         2.22810        0.94927  112.29s     Today, My wife chusagize stabbering imany parted far horror, and I was waking out guy docustomached in the car with a sportacial. Halthy decided to work by telling me what won't stop stopped $100 just and you ack on a table, just received into a with. FML\n",
      "     15         \u001b[36m1.99769\u001b[0m         2.07960        0.96061  111.71s     Today, I hadn after to push a bung on my burns of the head to school that it is a fall pass told her it hitting the left people to put relot hot head on the song. I started lawn. While told me that he's found they're off over the money. FML\n",
      "Skipped 90 sample(s) because the batch did not contain enough samples for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spaghetto.model.RNN at 0x7fd7e00bf6d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X, on_nth_batch=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to sample from the RNN using the _sample_ method. The first argument is the number of samples you want to get back. This method is unfortunately a little slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = rnn.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today, we watched it and killvas that nert work and turn and that looks like me walk. Horror, the nur had bugs. FML \n",
      "\n",
      "Today, my 5-year old daughter picked because birthday. FML\n",
      "\n",
      "Today, I discovered my potion months got banaled from his email punbous money that I've snaved myself his camera's. FML \n",
      "\n",
      "Today, while guitaring my morning advice with my time of walking girl. I lost bored, oddly buriover at a 2%, my boyfriend dumped me next to me. FML \n",
      "\n",
      "Today, thinking I gave hot both horribly dumping myself look going how had doesn't tell my book. And multing over my walle centre awake and be giving me blood camera my mom. FML \n",
      "\n",
      "Today, after a strearic, I'm in line that she'd ringzendy borrowed morning. The engagement ran video phone and said, \"Surry, she has been landing. \" FML \n",
      "\n",
      "Today, my ex girls asked me to move ever had been slowly on sticking my dream. Then my mom had been kissing our excuse for a high group. FML \n",
      "\n",
      "Today, on my body)ry, I moded stage midnight. After an employy, I googles'bed my ex. FML \n",
      "\n",
      "Today, I live in the room and note girls. Began laughing, and quickly put my dovelt, who got on for some food in all my back as, and have see \"noby!\" To get him in Net, uninion, now?\" FML \n",
      "\n",
      "Today, I yelled over for 11 months to move. My boyfriend. FML \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results already look quite nice but could certainly improve. Mainly, this could be achieved with more training data, but for this, we unfortunately have to wait. Further simple ways to improve the outcome could be:\n",
    "* train longer\n",
    "* use LSTMLayers instead of GRULayers\n",
    "* use more layers or a different architecture\n",
    "* use a different update rule or learning rate\n",
    "* use different embedding sizes or number of units\n",
    "* use dropout or other ways to regularize the net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN can be saved with _save_params_to_ so that we can later load it again to train some more or just create some fresh samples. To load the model, initialize it again and call _load_params_from_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn.save_params_to('../save/fmybotlife.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
